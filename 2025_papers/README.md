# 2025 年最喜欢的论文 PDF 链接合集

> 更新时间：2025-12-26；所有标记为“已验证”的链接均通过 HTTP HEAD 返回 200，确认可直接下载对应的 PDF。

## 1. Test Time Scaling
- [Test Time Scaling](https://arxiv.org/pdf/2508.03333v2.pdf) — 已验证
- [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/pdf/2506.07976v2.pdf) — 已验证
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948v1.pdf) — 已验证
- [s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393v3.pdf) — 已验证

### 尚未找到公开 PDF
- “Thinking” 到底是什么？（未检索到对应公开论文）

## 2. Efficient Reasoning
- [L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning](https://arxiv.org/pdf/2503.04697v2.pdf) — 已验证
- [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/pdf/2505.05315v2.pdf) — 已验证

### 尚未找到公开 PDF
- “如果思考一种 budget”（疑似主题句，未检索到对应论文）

## 3. Reasoning Analysis
- [DeepSeek-R1 Thoughtology: Let’s <think> about LLM reasoning](https://arxiv.org/pdf/2504.07128v2.pdf) — 已验证
- [Rethinking Reflection in Pre-Training](https://arxiv.org/pdf/2504.04022v1.pdf) — 已验证
- [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/pdf/2506.07976v2.pdf) — 已验证

### 尚未找到公开 PDF
- (How) Do reasoning models reason?（未检索到对应公开论文）

## 4. CLI Agent
- [SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering](https://arxiv.org/pdf/2405.15793v3.pdf) — 已验证

## 5. LLM × RL Agentic
- [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/pdf/2504.11536v2.pdf) — 已验证
- [ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](https://arxiv.org/pdf/2503.19470v3.pdf) — 已验证

### 尚未找到公开 PDF
- ReCall: Learning to Reason with Tool Call for LLMs via Reinforcement Learning（未检索到对应公开论文）
- “协议 token”（疑似概念词，未检索到对应论文）

## 6. Parallel Reasoning
- [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/pdf/2504.15466v2.pdf) — 已验证
- [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/pdf/2506.09991v2.pdf) — 已验证
- [Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding](https://arxiv.org/pdf/2502.11517v2.pdf) — 已验证

## 7. RL × Reasoning / RLVR
- [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/pdf/2504.20571v3.pdf) — 已验证
- [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/pdf/2505.03335v3.pdf) — 已验证
- [Learning to Reason without External Rewards](https://arxiv.org/pdf/2505.19590v2.pdf) — 已验证
- [Spurious Rewards: Rethinking Training Signals in RLVR](https://arxiv.org/pdf/2506.10947v1.pdf) — 已验证

## 8. Agent / Interaction / Deep Research Agent
- [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/pdf/2507.06229v5.pdf) — 已验证
- [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/pdf/2510.23601v1.pdf) — 已验证
- [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/pdf/2504.13171v1.pdf) — 已验证
- [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/pdf/2504.21776v2.pdf) — 已验证
- [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/pdf/2505.22648v3.pdf) — 已验证

## 9. Risk Modeling × LLM
- [Model Predictive Task Sampling for Efficient and Robust Adaptation](https://arxiv.org/pdf/2501.11039v6.pdf) — 已验证
- [Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?](https://arxiv.org/pdf/2507.04632v4.pdf) — 已验证

## 10. Multi-Agentic
- [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/pdf/2506.03136v2.pdf) — 已验证
- [CodeContests+: High-Quality Test Case Generation for Competitive Programming](https://arxiv.org/pdf/2506.05817v1.pdf) — 已验证

## 11. Sentient Agent
- [RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents](https://arxiv.org/pdf/2507.03112v1.pdf) — 已验证
- [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/pdf/2505.02847v3.pdf) — 已验证

## 12. LLM Security and Alignment
- [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/pdf/2507.11473v2.pdf) — 已验证
- [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/pdf/2502.17424v6.pdf) — 已验证
- [Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models](https://arxiv.org/pdf/2506.13206v2.pdf) — 已验证
- [Natural Emergent Misalignment from Reward Hacking in Production RL](https://arxiv.org/pdf/2511.18397v1.pdf) — 已验证
- [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/pdf/2512.09742v1.pdf) — 已验证

## 13. Model Steering
- [AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders](https://arxiv.org/pdf/2501.17148v3.pdf) — 已验证
- [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/pdf/2507.21509v3.pdf) — 已验证
- [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/pdf/2510.02453v1.pdf) — 已验证
